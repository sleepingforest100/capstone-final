{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:25:59.298324Z",
     "start_time": "2025-02-25T10:25:59.239641Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# question 4\n",
    "\n",
    "df = pd.read_csv('./data/Dataset.txt', header=None,\n",
    "                 names=['Player', 'Penalty', 'FreeKick', 'Corner', 'Target'])\n",
    "\n",
    "m = df.shape[0]\n",
    "# Extract features and target\n",
    "X = df[['Penalty', 'FreeKick', 'Corner']].values\n",
    "y = df['Target'].values\n",
    "\n",
    "# Add an intercept (bias) term (a column of ones)\n",
    "X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "\n",
    "# -----------------------------\n",
    "# Define helper functions\n",
    "# -----------------------------\n",
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the cost for linear regression.\n",
    "    NOTE: Here we use 1/m (not 1/(2*m)) so that the cost\n",
    "    matches the values in your reference table.\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    predictions = X.dot(theta)\n",
    "    cost = (1/m) * np.sum((predictions - y)**2)\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to learn theta.\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    for _ in range(iterations):\n",
    "        predictions = X.dot(theta)\n",
    "        error = predictions - y\n",
    "        # Gradient descent update\n",
    "        theta = theta - (alpha/m) * (X.T.dot(error))\n",
    "        cost_history.append(compute_cost(X, y, theta))\n",
    "    return theta, cost_history\n",
    "\n",
    "def run_experiment(iterations):\n",
    "    \"\"\"\n",
    "    Run gradient descent for a given number of iterations and\n",
    "    return the final cost, theta values, r2 score, and predictions.\n",
    "    \"\"\"\n",
    "    alpha = 0.1\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    theta, cost_history = gradient_descent(X, y, theta, alpha, iterations)\n",
    "    final_cost = cost_history[-1]\n",
    "    predictions = X.dot(theta)\n",
    "    r2 = r2_score(y, predictions)\n",
    "    return final_cost, theta, r2, predictions\n",
    "\n",
    "# -----------------------------\n",
    "# Run experiments for different iterations\n",
    "# -----------------------------\n",
    "results = {}\n",
    "for n in [1, 10, 100, 1000]:\n",
    "    cost, theta_vals, r2, preds = run_experiment(n)\n",
    "    results[n] = (cost, theta_vals, r2, preds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# question 5\n",
    "\n",
    "# We take the columns Penalty, FreeKick, Corner as features.\n",
    "X = df[['Penalty', 'FreeKick', 'Corner']].values\n",
    "y = df['Target'].values\n",
    "\n",
    "# Number of training examples\n",
    "m = len(y)\n",
    "\n",
    "# Add a column of ones for the intercept term\n",
    "# so that X becomes [1, Penalty, FreeKick, Corner]\n",
    "X = np.hstack([np.ones((m, 1)), X])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Compute theta using the Normal Equation:\n",
    "#    theta = (X^T * X)^(-1) * X^T * y\n",
    "# ---------------------------------------------------------------------\n",
    "# Note: In practice, you may want to use a pseudo-inverse for numerical stability.\n",
    "theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Make predictions and compute r^2 score\n",
    "# ---------------------------------------------------------------------\n",
    "predictions = X @ theta\n",
    "r2 = r2_score(y, predictions)\n",
    "\n",
    "\n",
    "# question 4 results\n",
    "print(\"{:<12} {:<14} {:<40} {:<8}\".format(\n",
    "    \"# iterations\",\n",
    "    \"Cost Function\",\n",
    "    \"Optimal Values of Theta\",\n",
    "    \"r2_score\"\n",
    "))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Data rows\n",
    "for n in [1, 10, 100, 1000]:\n",
    "    cost, theta_vals, r2, _ = results[n]\n",
    "    cost_str = f\"{cost:.3f}\"\n",
    "    theta_str = \", \".join([f\"{theta:.3f}\" for theta in theta_vals])\n",
    "    r2_str = f\"{r2:.3f}\"\n",
    "    print(f\"n = {n:<10} {cost_str:<14} {theta_str:<40} {r2_str:<8}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Determine the player with the maximum predicted value after 1000 iterations\n",
    "# -----------------------------\n",
    "_, theta_1000, _, preds_1000 = results[1000]\n",
    "max_index = np.argmax(preds_1000)\n",
    "player_max = df.iloc[max_index]['Player']\n",
    "print(f\"\\nWho has a maximum predicted value after 1000 iterations? {player_max}\")\n",
    "\n",
    "\n",
    "# question 5 results\n",
    "print(\"Optimal values of theta (rounded to 3 decimals):\")\n",
    "for i, val in enumerate(theta):\n",
    "    print(f\"theta{i} = {val:.3f}\")\n",
    "\n",
    "print(f\"\\nr2-score value (rounded to 3 decimals): {r2:.3f}\")"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m r2_score\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# question 4\u001B[39;00m\n\u001B[0;32m      7\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data/Dataset.txt\u001B[39m\u001B[38;5;124m'\u001B[39m, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m      8\u001B[0m                  names\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPlayer\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPenalty\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFreeKick\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCorner\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTarget\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'sklearn'"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
